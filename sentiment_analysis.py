# -*- coding: utf-8 -*-
"""Sentiment_updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EsgFUeT5IGDYidMmGMs3oszYf8-9E8By
"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

# Load dataset
data = pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')

# Assume 'data' is a DataFrame with 'review' and 'sentiment' columns
X = data['review']
y = data['sentiment']

# Define pipeline for preprocessing and model training
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', MultinomialNB())
])

# Define hyperparameters to search over for MultinomialNB
parameters_nb = {
    'tfidf__max_features': [1000, 2000, 3000],
    'tfidf__ngram_range': [(1, 1), (1, 2)],
    'clf__alpha': [0.1, 0.5, 1.0]  # Hyperparameter for MultinomialNB
}

# Define hyperparameters to search over for RandomForestClassifier
parameters_rf = {
    'tfidf__max_features': [1000, 2000, 3000],
    'tfidf__ngram_range': [(1, 1), (1, 2)],
    #'clf__n_estimators': [50, 100, 200]  # Hyperparameter for RandomForestClassifier
}

# Define hyperparameters to search over for LogisticRegression
parameters_lr = {
    'tfidf__max_features': [1000, 2000, 3000],
    'tfidf__ngram_range': [(1, 1), (1, 2)],
    #'clf__C': [0.1, 1, 10]  # Hyperparameter for LogisticRegression
}

# Define classifiers and their respective parameter grids
classifiers = [MultinomialNB(), RandomForestClassifier(), LogisticRegression()]
parameters = [parameters_nb, parameters_rf, parameters_lr]

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Perform grid search with cross-validation for each classifier
best_models = []
for clf, params in zip(classifiers, parameters):
    grid_search = GridSearchCV(pipeline, params, cv=5, n_jobs=-1, verbose=1)
    grid_search.fit(X_train, y_train)
    best_models.append(grid_search.best_estimator_)

    # Print best parameters and best score for each classifier
    print("Best Parameters for {}: {}".format(clf.__class__.__name__, grid_search.best_params_))
    print("Best Score for {}: {}".format(clf.__class__.__name__, grid_search.best_score_))

    # Evaluate best model on test set
    y_pred = best_models[-1].predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy for {}: {}".format(clf.__class__.__name__, accuracy))
    print(classification_report(y_test, y_pred))

